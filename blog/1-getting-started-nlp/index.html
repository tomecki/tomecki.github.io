<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=favicon.png rel=icon type=image/png> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-119904159-1"></script> <script> window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-119904159-1'); </script> <link href=client/main.38969126.css rel=stylesheet><link href=client/[slug].6b835b8c.css rel=stylesheet><link href=client/client.e20925cb.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Getting started with Natural Language Processing in Polish</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-x6z7kg><ul class=svelte-x6z7kg><li class=svelte-x6z7kg><a href=. class=svelte-x6z7kg>home</a></li> <li class=svelte-x6z7kg><a href=about class=svelte-x6z7kg>about</a></li> <li class=svelte-x6z7kg><a href=blog class=svelte-x6z7kg aria-current=page rel=prefetch>blog</a></ul></nav> <main class=svelte-1uhnsl8> <div class="svelte-1e37h3z header"><img alt="Getting started with Natural Language Processing in Polish" src=images/1-assets/header.jpg class=svelte-1e37h3z></div> <h1>Getting started with Natural Language Processing in Polish</h1> <div class="svelte-1e37h3z content"><p>There are countless tutorials on how to do NLP on the internet. Why would you care to read another one? Well, we will be looking into analysing Polish language, which is still rather underdeveloped in comparison to English. </p> <p>Polish language is much more complex when compared to English language, as it contains 7 cases (<em>deklinacja</em>), 3 kinds (masculine, femenine, unspecified) and 11 different templates of verb conjugation, not to mention the exceptions <a href=#cite-1>[1]</a> <a href=#cite-2>[2]</a> <a href=#cite-3>[3]</a>. English is rather simple: verb conjugation is easy, nouns pluralise easily, there are hardly any genders.</p> <h2 id=corpus>Corpus</h2> <p>For the sake of this analysis, we will look at collection of articles obtained (the nice word for <em>scraped</em>) from one of the leading right-wing news websites, <a href=http://wpolityce.pl>wpolityce.pl</a>.</p> <p><img alt="Sample article" src=/images/1-assets/fullarticle.png> Sample article from the dataset.</p> <p>The articles were collected over a period of 15 days ending 10th June 2019, only articles linked from front page were collected. In total there are 1453 unique articles. I shall later publish my insights on collecting datasets from the wild.</p> <h2 id=tokenizing>Tokenizing</h2> <p>Tokens are the smallest unit of the language that's most commonly taken into account when doing text analysis. You may think of it as a word, which in most cases it is. Things get trickier in English with it's abbreviations like <em>I'd</em> or <em>we've</em> but Polish is much easier in this aspect.</p> <pre class=language-javascriptreact><code>„|Gazeta|Wyborcza|”|wielokrotnie|na| |swoich|łamach|wspierała|działania|Fundacji|Nie|lękajcie|się|Marka|Lisińskiego|.</code></pre><p>Sample tokenization of a sentence from the corpus, tokens are delimited with vertical bars. Please note empty token between words <em>na</em> and <em>swoich</em> - it's a dirty whitespace, which we'll deal with shortly.</p> <p>In Python, you can tokenize using either NLTK's <a href=https://www.nltk.org/api/nltk.tokenize.html>nltk.tokenize</a> package or spaCy's <a href=https://spacy.io/api/tokenizer>tokenizer</a> using pretrained language models. I have used the latter with <a href=https://spacy.io/models/xx#xx_ent_wiki_sm>xx_ent_wiki_sm</a> multilanguage model. In empirical analysis it appeared to split up the tokens in a plausible manner.</p> <p><img alt="The distribution of numbers of tokens for an article looks as follows:
" src=/images/1-assets/article-token-length.png></p> <p>The distribution of numbers of tokens in an article: vertical axis shows number of articles falling into particular bucket, horizontal axis is the token lenght of article.</p> <h2 id=quantitative-analysis-corpus>Quantitative analysis: corpus</h2> <p>Reading the entire dataset without any cleaning yields mediocre results. The resulting corpus contains:</p> <ul> <li>badly interpreted non-breaking space from latin1 encoding (<code>"\xa0"</code>)</li> <li>whitespace-only tokens (<code>"\n"</code>, <code>"\n \n"</code> and others)</li> <li>punctuation, also in repeated non-ambiguous way (multiple quote chars, hyphen/minus used interchangeably)</li> <li>stopwords: common words that skew the word distribution but do not bring any value to the analysis.</li> </ul> <h3 id=frequent-words-unigrams>Frequent words (unigrams)</h3> <p><img alt="10 most frequent tokens in a clean dataset" src=/images/1-assets/clean-token-frequency.png></p> <table> <thead> <tr> <th>word</th> <th>frequency</th> </tr> </thead> <tr> <td>PiS</td> <td>1444</td> </tr> <tr> <td>proc</td> <td>1198</td> </tr> <tr> <td>wyborach</td> <td>835</td> </tr> <tr> <td>powiedział</td> <td>726</td> </tr> <tr> <td>PAP</td> <td>709</td> </tr> <tr> <td>PSL</td> <td>676</td> </tr> <tr> <td>r.</td> <td>653</td> </tr> <tr> <td>Polski</td> <td>652</td> </tr> <tr> <td>Polsce</td> <td>625</td> </tr> <tr> <td>Europejskiej</td> <td>601</td> </tr> </table> <h4 id=word-cloud>Word cloud</h4> <p>The raw data above can be visualised using a word cloud. The idea is to represent the frequency of the word with it's relative size on the visualisation, framing it in visually attractive and appealing way.</p> <p>You can find plenty of online generators that will easily let you tweak the shape, colors and fonts. Check out my wordcloud of 50 most common words from the corpus generated using generator from <a href=https://worditout.com>worditout.com</a> <img alt="Sample wordcloud" src=/images/1-assets/wordcloud.png></p> <p>Concluding our unigram analysis, the most common topics were revolving around PiS (abbreviation used by the polish ruling party), elections (<em>wyborach</em> stands for elections, <em>proc.</em> is abbreviated percent).</p> <h3 id=ngrams>Ngrams</h3> <p>Ngram is just a sequence of <em>n</em> consecutive tokens. For <em>n=2</em> we also use word <em>bigram</em>. You can get the most frequent Ngrams (sequences of words of particilar length) using either <code>nltk.ngrams</code> or trivial custom code. For length 2-3 you are highly likely to fetch popular full names. Longer ngrams are likely to catch parts of frequently repeated sentences, like promos, ads and references.</p> <h4 id=bigrams>Bigrams</h4> <p>As expected, we are catching frequent proper names of entities in the text. Below is a breakdown of 10 most frequent bigrams from the corpus:</p> <table> <thead> <tr> <th>frequency</th> <th>bigram</th> <th>interpretation</th> </tr> </thead> <tr> <td>377</td> <td>Koalicji Europejskiej</td> <td>political party</td> </tr> <tr> <td>348</td> <td>Parlamentu Europejskiego</td> <td>name of institution</td> </tr> <tr> <td>322</td> <td>PAP EOT</td> <td>source/author alias</td> </tr> <tr> <td>261</td> <td>4 czerwca</td> <td>important date discussed at the time</td> </tr> <tr> <td>223</td> <td>Koalicja Europejska</td> <td>political party</td> </tr> <tr> <td>136</td> <td>Andrzej Duda</td> <td>full name of President of Poland</td> </tr> <tr> <td>126</td> <td>Jarosław Kaczyński</td> <td>full name of important politician</td> </tr> <tr> <td>126</td> <td>Jana Pawła</td> <td>most likely: prefix from <em>John Paul II</em></td> </tr> <tr> <td>125</td> <td>Pawła II</td> <td>most likely: suffix from <em>John Paul II</em></td> </tr> </table> <h4 id=sixgrams>Sixgrams</h4> <p>Longer ngrams expose frequent phrases used throughout the corpus. Polish has a lot of cases and persons, so this method is not of much help to find frequent phrases in the language itself; you are more likely to find repeated parts of sentences or adverts.</p> <table> <thead> <tr> <th>frequency</th> <th>sixgram</th> </tr> </thead> <tr> <td>61</td> <td>Kup nasze pismo w kiosku lub</td> </tr> <tr> <td>61</td> <td>nasze pismo w kiosku lub skorzystaj</td> </tr> <tr> <td>61</td> <td>pismo w kiosku lub skorzystaj z</td> </tr> <tr> <td>61</td> <td>w kiosku lub skorzystaj z bardzo</td> </tr> <tr> <td>61</td> <td>kiosku lub skorzystaj z bardzo wygodnej</td> </tr> </table> <p>Clearly, there is an issue here. While doing our quantitative analysis our distributions and counts are skewed by the advert texts being present in most of the articles. Ideally we should factor these phrases out of our corpus, either at a stage of data collection (improving the parser to annotate/omit these phrase) or data preprocessing (something we're doing in this article).</p> <p>Much better idea is to perform ngrams analysis on lemmatized text. This way you may mine more knowledge about the language, not just repeated phrases.</p> <h2 id=morphological-analysis>Morphological analysis</h2> <p>In the previous paragraph we saw that the different forms of the words will make learning about the language harder - we are capturing the information about the entire word, with the particular gender, tense and case. This becomes particularly disruptive in Polish. </p> <p>One potential solution is called <em>morphological analysis</em>. This is a process of mapping a word to all of it's potential dictionary base words (<em>lexems</em>). Sample lemmatization:</p> <table> <thead> <tr> <th>Original word</th> <th>Lemma tag</th> <th>Lexem</th> </tr> </thead> <tr> <td>Dyrektywa</td> <td>dyrektywa</td> <td>subst:sg:nom:f</td> </tr> <tr> <td>PSD2</td> <td>PSD2</td> <td>ign</td> </tr> <tr> <td>zawiera</td> <td>zawierać</td> <td>fin:sg:ter:imperf</td> </tr> <tr> <td>przepisy</td> <td>przepis</td> <td>subst:pl:nom:m3</td> </tr> <tr> <td>odnoszące</td> <td>odnosić</td> <td>pact:sg:nom:n:imperf:aff</td> </tr> <tr> <td>się</td> <td>się</td> <td>qub</td> </tr> <tr> <td>do</td> <td>do</td> <td>prep:gen</td> </tr> <tr> <td>płatności</td> <td>płatność</td> <td>subst:sg:gen:f</td> </tr> <tr> <td>elektronicznych</td> <td>elektroniczny</td> <td>adj:pl:gen:m1:pos</td> </tr> <tr> <td>realizowanych</td> <td>realizować</td> <td>ppas:pl:gen:m1:imperf:aff</td> </tr> <tr> <td>wewnątrz</td> <td>wewnątrz</td> <td>adv:pos</td> </tr> <tr> <td>Unii</td> <td>unia</td> <td>subst:sg:gen:f</td> </tr> <tr> <td>Europejskiej</td> <td>europejski</td> <td>adj:sg:gen:f:pos</td> </tr> <tr> <td>.</td> <td>.</td> <td>interp</td> </tr> </table> <p>We will not be implementing this part from scratch, instead we can use resources from IPI PAN (Polish Academy of Science), specifically a tool called <a href=http://morfeusz.sgjp.pl/ >Morfeusz</a>. </p> <p>Morfeusz is a morphosyntactic analyzer which you can use to find all word lexems and the forms. The output will be slightly different than the table above: instead, for every input word Morfeusz will output all of it's possible dictionary forms.</p> <h3 id=usage>Usage</h3> <p>I assume you are running either a recent Ubuntu or Fedora. After fetching the right version from <a href=http://morfeusz.sgjp.pl/download/ >Morfeusz download page</a> do the following:</p> <pre class=language-javascriptreact><code>tar xzfv &lt;path to archive>
sudo cp morfeusz/lib/libmorfeusz2.so /usr/lib/<span class=hljs-built_in>local</span>
sudo chmod a+x /usr/lib/<span class=hljs-built_in>local</span>/libmorfeusz2.so
sudo <span class=hljs-built_in>echo</span> <span class=hljs-string>"/usr/local/lib"</span> > /etc/ld.so.conf.d/local.conf
sudo ldconfig</code></pre><p>Now we can download and install the python egg from morfeusz download page. </p> <pre class=language-javascriptreact><code>easy_install &lt;path_to_downloaded_egg></code></pre><p>The best thing about the python package is that you can retrieve the result from the analyzer using just a couple of python lines:</p> <pre class=language-javascriptreact><code><span class=hljs-keyword>import</span> morfeusz2
m = morfeusz2.Morfeusz()
result = m.analyze(<span class=hljs-string>"Ala ma kota, kot ma Alę."</span>)
<span class=hljs-comment># the result is:</span>
[(<span class=hljs-number>0</span>, <span class=hljs-number>1</span>, (<span class=hljs-string>'Dyrektywa'</span>, <span class=hljs-string>'dyrektywa'</span>, <span class=hljs-string>'subst:sg:nom:f'</span>, [<span class=hljs-string>'nazwa_pospolita'</span>], [])),   
 (<span class=hljs-number>1</span>, <span class=hljs-number>2</span>, (<span class=hljs-string>'PSD2'</span>, <span class=hljs-string>'PSD2'</span>, <span class=hljs-string>'ign'</span>, [], [])),                                         
 (<span class=hljs-number>2</span>, <span class=hljs-number>3</span>, (<span class=hljs-string>'zawiera'</span>, <span class=hljs-string>'zawierać'</span>, <span class=hljs-string>'fin:sg:ter:imperf'</span>, [], [])),                    
 (<span class=hljs-number>3</span>,                                                                              
  <span class=hljs-number>4</span>,                                                                              
  (<span class=hljs-string>'przepisy'</span>, <span class=hljs-string>'przepis'</span>, <span class=hljs-string>'subst:pl:nom.acc.voc:m3'</span>, [<span class=hljs-string>'nazwa_pospolita'</span>], [])),   
 (<span class=hljs-number>4</span>,                                     
  <span class=hljs-number>5</span>,                                     
  (<span class=hljs-string>'odnoszące'</span>,                                                                   
   <span class=hljs-string>'odnosić'</span>,                                                                     
   <span class=hljs-string>'pact:pl:nom.acc.voc:m2.m3.f.n:imperf:aff'</span>,                                    
   [],                                                                            
   [])),                                 
 (<span class=hljs-number>4</span>, <span class=hljs-number>5</span>, (<span class=hljs-string>'odnoszące'</span>, <span class=hljs-string>'odnosić'</span>, <span class=hljs-string>'pact:sg:nom.acc.voc:n:imperf:aff'</span>, [], [])),    </code></pre><p>Putting it all in a pandas DataFrame is a trivial task as well:</p> <pre class=language-javascriptreact><code><span class=hljs-keyword>import</span> pandas <span class=hljs-keyword>as</span> pd
colnames = [<span class=hljs-string>"token_start"</span>, <span class=hljs-string>"token_end"</span>, <span class=hljs-string>"segment"</span>, <span class=hljs-string>"lemma"</span>, <span class=hljs-string>"interp"</span>, <span class=hljs-string>"common"</span>, <span class=hljs-string>"qualifiers"</span>]
df = pd.DataFrame.from_records([(elem[<span class=hljs-number>0</span>], elem[<span class=hljs-number>1</span>], *elem[<span class=hljs-number>2</span>]) <span class=hljs-keyword>for</span> elem <span class=hljs-keyword>in</span> result], columns=colnames)</code></pre><p><img alt="" src=/images/1-assets/pandas_df.png></p> <p>Helpful links on tackling Morfeusz:</p> <ul> <li><a href=http://download.sgjp.pl/morfeusz/Morfeusz2.pdf>Full docs on Morfeusz</a></li> <li><a href=http://www.ipipan.waw.pl/~wolinski/publ/znakowanie.pdf>Meaning of the morphosyntactic tags</a></li> </ul> <h3 id=lemmatization>Lemmatization</h3> <p>You have already seen the output of a lemmatizer at the beginning of previous section. In contrast to Morfeusz's output, this time we want to obtain a mapping for each word to it's most probable dictionary form.</p> <p>In order to obtain direct tags instead of list of tags for each token, you will need to go deep into morphological taggers land. Your options are:</p> <ul> <li><a href=http://nlp.pwr.wroc.pl/redmine/projects/wcrft/wiki/ >WCRFT</a> - actually having a working demo <a href=http://ws.clarin-pl.eu/tager.shtml>here</a>. Requires some skill to get it to work, dependencies are non-trivial to put together,</li> <li><a href=http://zil.ipipan.waw.pl/PANTERA>PANTERA</a> - doesn't appear to be actively maintained</li> <li><a href=https://github.com/kawu/concraft-pl>Concraft</a> - implemented in Haskell, looks to be the easiest one to get running</li> </ul> <p>If you are just doing a casual analysis of text, you may want to consider outsourcing the whole tagging process to a RESTful API, like <a href=http://nlp.pwr.wroc.pl/redmine/projects/nlprest2/wiki/Asynapi>http://nlp.pwr.wroc.pl/redmine/projects/nlprest2/wiki/Asynapi</a>. </p> <p>You can easily reverse-engineer the exact calls to the API using Firefox/Chrome dev tools. This way you can find the correct values for the parameters. <img alt=devtools src=/images/1-assets/devtools.png></p> <p>As with any API provided to the public, you should notify the provider about your intentions and remember about some sane throttling of requests. Moreover, it's considered good manners to set the user parameter to something meaningful (like email address).</p> <h2 id=conclusions>Conclusions</h2> <p>We went through the basic steps of analyzing a corpus and touched on more advanced topics, like morphological analysis and lemmatization. If you're doing a quick and dirty analysis, outsourcing those basic tasks to ready tools and APIs sounds like the best option. In case you need to run a large scale experiments, it makes more sense to run the tools on your own hardware. The lemmatized text might become a powerful input to a more sophisticated models, like recurrent neural networks. </p> <h2 id=citations>Citations</h2> <ul> <li>[1] <span id=cite-1></span><a href=http://grzegorz.jagodzinski.prv.pl/gram/pl/gram01.html>http://grzegorz.jagodzinski.prv.pl: Polish grammar rules</a></li> <li>[2] <span id=cite-2></span><a href=https://pl.wikipedia.org/wiki/Gramatyka_j%C4%99zyka_polskiego>Wikpedia: Polish grammar</a></li> <li>[3] <span id=cite-3></span><a href=https://www.quora.com/How-many-words-are-there-in-the-Polish-language>Quora: how many words are the in the Polish language</a></li> </ul> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{html:"\u003Cp\u003EThere are countless tutorials on how to do NLP on the internet. Why would you care to read another one? Well, we will be looking into analysing Polish language, which is still rather underdeveloped in comparison to English. \u003C\u002Fp\u003E\n\u003Cp\u003EPolish language is much more complex when compared to English language, as it contains 7 cases  (\u003Cem\u003Edeklinacja\u003C\u002Fem\u003E), 3 kinds (masculine, femenine, unspecified) and 11 different templates of verb conjugation, not to mention the exceptions \u003Ca href=\"#cite-1\"\u003E[1]\u003C\u002Fa\u003E \u003Ca href=\"#cite-2\"\u003E[2]\u003C\u002Fa\u003E \u003Ca href=\"#cite-3\"\u003E[3]\u003C\u002Fa\u003E. English is rather simple: verb conjugation is easy, nouns pluralise easily, there are hardly any genders.\u003C\u002Fp\u003E\n\u003Ch2 id=\"corpus\"\u003ECorpus\u003C\u002Fh2\u003E\n\u003Cp\u003EFor the sake of this analysis, we will look at collection of articles obtained (the nice word for \u003Cem\u003Escraped\u003C\u002Fem\u003E) from one of the leading right-wing news websites, \u003Ca href=\"http:\u002F\u002Fwpolityce.pl\"\u003Ewpolityce.pl\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fimages\u002F1-assets\u002Ffullarticle.png\" alt=\"Sample article\"\u003E\nSample article from the dataset.\u003C\u002Fp\u003E\n\u003Cp\u003EThe articles were collected over a period of 15 days ending 10th June 2019, only articles linked from front page were collected. In total there are 1453 unique articles. I shall later publish my insights on collecting datasets from the wild.\u003C\u002Fp\u003E\n\u003Ch2 id=\"tokenizing\"\u003ETokenizing\u003C\u002Fh2\u003E\n\u003Cp\u003ETokens are the smallest unit of the language that&#39;s most commonly taken into account when doing text analysis. You may think of it as a word, which in most cases it is. Things get trickier in English with it&#39;s abbreviations like \u003Cem\u003EI&#39;d\u003C\u002Fem\u003E or \u003Cem\u003Ewe&#39;ve\u003C\u002Fem\u003E but Polish is much easier in this aspect.\u003C\u002Fp\u003E\n\u003Cpre class='language-javascriptreact'\u003E\u003Ccode\u003E„|Gazeta|Wyborcza|”|wielokrotnie|na| |swoich|łamach|wspierała|działania|Fundacji|Nie|lękajcie|się|Marka|Lisińskiego|.\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003ESample tokenization of a sentence from the corpus, tokens are delimited with vertical bars. Please note empty token between words \u003Cem\u003Ena\u003C\u002Fem\u003E and \u003Cem\u003Eswoich\u003C\u002Fem\u003E - it&#39;s a dirty whitespace, which we&#39;ll deal with shortly.\u003C\u002Fp\u003E\n\u003Cp\u003EIn Python, you can tokenize using either NLTK&#39;s \u003Ca href=\"https:\u002F\u002Fwww.nltk.org\u002Fapi\u002Fnltk.tokenize.html\"\u003Enltk.tokenize\u003C\u002Fa\u003E package or spaCy&#39;s \u003Ca href=\"https:\u002F\u002Fspacy.io\u002Fapi\u002Ftokenizer\"\u003Etokenizer\u003C\u002Fa\u003E using pretrained language models. I have used the latter with \u003Ca href=\"https:\u002F\u002Fspacy.io\u002Fmodels\u002Fxx#xx_ent_wiki_sm\"\u003Exx_ent_wiki_sm\u003C\u002Fa\u003E multilanguage model. In empirical analysis it appeared to split up the tokens in a plausible manner.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fimages\u002F1-assets\u002Farticle-token-length.png\" alt=\"The distribution of numbers of tokens for an article looks as follows:\n\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EThe distribution of numbers of tokens in an article: vertical axis shows number of articles falling into particular bucket, horizontal axis is the token lenght of article.\u003C\u002Fp\u003E\n\u003Ch2 id=\"quantitative-analysis-corpus\"\u003EQuantitative analysis: corpus\u003C\u002Fh2\u003E\n\u003Cp\u003EReading the entire dataset without any cleaning yields mediocre results. The resulting corpus contains:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003Ebadly interpreted non-breaking space from latin1 encoding (\u003Ccode\u003E&quot;\\xa0&quot;\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\n\u003Cli\u003Ewhitespace-only tokens (\u003Ccode\u003E&quot;\\n&quot;\u003C\u002Fcode\u003E, \u003Ccode\u003E&quot;\\n \\n&quot;\u003C\u002Fcode\u003E and others)\u003C\u002Fli\u003E\n\u003Cli\u003Epunctuation, also in repeated non-ambiguous way (multiple quote chars, hyphen\u002Fminus used interchangeably)\u003C\u002Fli\u003E\n\u003Cli\u003Estopwords: common words that skew the word distribution but do not bring any value to the analysis.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"frequent-words-unigrams\"\u003EFrequent words (unigrams)\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fimages\u002F1-assets\u002Fclean-token-frequency.png\" alt=\"10 most frequent tokens in a clean dataset\"\u003E\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003Eword\u003C\u002Fth\u003E\n\u003Cth\u003Efrequency\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EPiS\u003C\u002Ftd\u003E\n\u003Ctd\u003E1444\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eproc\u003C\u002Ftd\u003E\n\u003Ctd\u003E1198\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ewyborach\u003C\u002Ftd\u003E\n\u003Ctd\u003E835\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Epowiedział\u003C\u002Ftd\u003E\n\u003Ctd\u003E726\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EPAP\u003C\u002Ftd\u003E\n\u003Ctd\u003E709\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EPSL\u003C\u002Ftd\u003E\n\u003Ctd\u003E676\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Er.\u003C\u002Ftd\u003E\n\u003Ctd\u003E653\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EPolski\u003C\u002Ftd\u003E\n\u003Ctd\u003E652\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EPolsce\u003C\u002Ftd\u003E\n\u003Ctd\u003E625\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EEuropejskiej\u003C\u002Ftd\u003E\n\u003Ctd\u003E601\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Ch4 id=\"word-cloud\"\u003EWord cloud\u003C\u002Fh4\u003E\n\u003Cp\u003EThe raw data above can be visualised using a word cloud. The idea is to represent the frequency of the word with it&#39;s relative size on the visualisation, framing it in visually attractive and appealing way.\u003C\u002Fp\u003E\n\u003Cp\u003EYou can find plenty of online generators that will easily let you tweak the shape, colors and fonts. Check out my wordcloud of 50 most common words from the corpus generated using generator from \u003Ca href=\"https:\u002F\u002Fworditout.com\"\u003Eworditout.com\u003C\u002Fa\u003E\n\u003Cimg src=\"\u002Fimages\u002F1-assets\u002Fwordcloud.png\" alt=\"Sample wordcloud\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EConcluding our unigram analysis, the most common topics were revolving around PiS (abbreviation used by the polish ruling party), elections (\u003Cem\u003Ewyborach\u003C\u002Fem\u003E stands for elections, \u003Cem\u003Eproc.\u003C\u002Fem\u003E is abbreviated percent).\u003C\u002Fp\u003E\n\u003Ch3 id=\"ngrams\"\u003ENgrams\u003C\u002Fh3\u003E\n\u003Cp\u003ENgram is just a sequence of \u003Cem\u003En\u003C\u002Fem\u003E consecutive tokens. For \u003Cem\u003En=2\u003C\u002Fem\u003E we also use word \u003Cem\u003Ebigram\u003C\u002Fem\u003E. You can get the most frequent Ngrams (sequences of words of particilar length) using either \u003Ccode\u003Enltk.ngrams\u003C\u002Fcode\u003E or trivial custom code. For length 2-3 you are highly likely to fetch popular full names. Longer ngrams are likely to catch parts of frequently repeated sentences, like promos, ads and references.\u003C\u002Fp\u003E\n\u003Ch4 id=\"bigrams\"\u003EBigrams\u003C\u002Fh4\u003E\n\u003Cp\u003EAs expected, we are catching frequent proper names of entities in the text. Below is a breakdown of 10 most frequent bigrams from the corpus:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003Efrequency\u003C\u002Fth\u003E\n\u003Cth\u003Ebigram\u003C\u002Fth\u003E\n\u003Cth\u003Einterpretation\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003E377\u003C\u002Ftd\u003E\n\u003Ctd\u003EKoalicji Europejskiej\u003C\u002Ftd\u003E\n\u003Ctd\u003Epolitical party\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E348\u003C\u002Ftd\u003E\n\u003Ctd\u003EParlamentu Europejskiego\u003C\u002Ftd\u003E\n\u003Ctd\u003Ename of institution\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E322\u003C\u002Ftd\u003E\n\u003Ctd\u003EPAP EOT\u003C\u002Ftd\u003E\n\u003Ctd\u003Esource\u002Fauthor alias\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E261\u003C\u002Ftd\u003E\n\u003Ctd\u003E4 czerwca\u003C\u002Ftd\u003E\n\u003Ctd\u003Eimportant date discussed at the time\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E223\u003C\u002Ftd\u003E\n\u003Ctd\u003EKoalicja Europejska\u003C\u002Ftd\u003E\n\u003Ctd\u003Epolitical party\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E136\u003C\u002Ftd\u003E\n\u003Ctd\u003EAndrzej Duda\u003C\u002Ftd\u003E\n\u003Ctd\u003Efull name of President of Poland\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E126\u003C\u002Ftd\u003E\n\u003Ctd\u003EJarosław Kaczyński\u003C\u002Ftd\u003E\n\u003Ctd\u003Efull name of important politician\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E126\u003C\u002Ftd\u003E\n\u003Ctd\u003EJana Pawła\u003C\u002Ftd\u003E\n\u003Ctd\u003Emost likely: prefix from \u003Cem\u003EJohn Paul II\u003C\u002Fem\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E125\u003C\u002Ftd\u003E\n\u003Ctd\u003EPawła II\u003C\u002Ftd\u003E\n\u003Ctd\u003Emost likely: suffix from \u003Cem\u003EJohn Paul II\u003C\u002Fem\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Ch4 id=\"sixgrams\"\u003ESixgrams\u003C\u002Fh4\u003E\n\u003Cp\u003ELonger ngrams expose frequent phrases used throughout the corpus. Polish has a lot of cases and persons, so this method is not of much help to find frequent phrases in the language itself; you are more likely to find repeated parts of sentences or adverts.\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003Efrequency\u003C\u002Fth\u003E\n\u003Cth\u003Esixgram\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003E61\u003C\u002Ftd\u003E\n\u003Ctd\u003EKup nasze pismo w kiosku lub\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E61\u003C\u002Ftd\u003E\n\u003Ctd\u003Enasze pismo w kiosku lub skorzystaj\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E61\u003C\u002Ftd\u003E\n\u003Ctd\u003Epismo w kiosku lub skorzystaj z\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E61\u003C\u002Ftd\u003E\n\u003Ctd\u003Ew kiosku lub skorzystaj z bardzo\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E61\u003C\u002Ftd\u003E\n\u003Ctd\u003Ekiosku lub skorzystaj z bardzo wygodnej\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003EClearly, there is an issue here. While doing our quantitative analysis our distributions and counts are skewed by the advert texts being present in most of the articles. Ideally we should factor these phrases out of our corpus, either at a stage of data collection (improving the parser to annotate\u002Fomit these phrase) or data preprocessing (something we&#39;re doing in this article).\u003C\u002Fp\u003E\n\u003Cp\u003EMuch better idea is to perform ngrams analysis on lemmatized text. This way you may mine more knowledge about the language, not just repeated phrases.\u003C\u002Fp\u003E\n\u003Ch2 id=\"morphological-analysis\"\u003EMorphological analysis\u003C\u002Fh2\u003E\n\u003Cp\u003EIn the previous paragraph we saw that the different forms of the words will make learning about the language harder - we are capturing the information about the entire word, with the particular gender, tense and case. This becomes particularly disruptive in Polish. \u003C\u002Fp\u003E\n\u003Cp\u003EOne potential solution is called \u003Cem\u003Emorphological analysis\u003C\u002Fem\u003E. This is a process of mapping a word to all of it&#39;s potential dictionary base words (\u003Cem\u003Elexems\u003C\u002Fem\u003E). Sample lemmatization:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EOriginal word\u003C\u002Fth\u003E\n\u003Cth\u003ELemma tag\u003C\u002Fth\u003E\n\u003Cth\u003ELexem\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EDyrektywa\u003C\u002Ftd\u003E\n\u003Ctd\u003Edyrektywa\u003C\u002Ftd\u003E\n\u003Ctd\u003Esubst:sg:nom:f\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EPSD2\u003C\u002Ftd\u003E\n\u003Ctd\u003EPSD2\u003C\u002Ftd\u003E\n\u003Ctd\u003Eign\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ezawiera\u003C\u002Ftd\u003E\n\u003Ctd\u003Ezawierać\u003C\u002Ftd\u003E\n\u003Ctd\u003Efin:sg:ter:imperf\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eprzepisy\u003C\u002Ftd\u003E\n\u003Ctd\u003Eprzepis\u003C\u002Ftd\u003E\n\u003Ctd\u003Esubst:pl:nom:m3\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eodnoszące\u003C\u002Ftd\u003E\n\u003Ctd\u003Eodnosić\u003C\u002Ftd\u003E\n\u003Ctd\u003Epact:sg:nom:n:imperf:aff\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Esię\u003C\u002Ftd\u003E\n\u003Ctd\u003Esię\u003C\u002Ftd\u003E\n\u003Ctd\u003Equb\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Edo\u003C\u002Ftd\u003E\n\u003Ctd\u003Edo\u003C\u002Ftd\u003E\n\u003Ctd\u003Eprep:gen\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Epłatności\u003C\u002Ftd\u003E\n\u003Ctd\u003Epłatność\u003C\u002Ftd\u003E\n\u003Ctd\u003Esubst:sg:gen:f\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eelektronicznych\u003C\u002Ftd\u003E\n\u003Ctd\u003Eelektroniczny\u003C\u002Ftd\u003E\n\u003Ctd\u003Eadj:pl:gen:m1:pos\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Erealizowanych\u003C\u002Ftd\u003E\n\u003Ctd\u003Erealizować\u003C\u002Ftd\u003E\n\u003Ctd\u003Eppas:pl:gen:m1:imperf:aff\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ewewnątrz\u003C\u002Ftd\u003E\n\u003Ctd\u003Ewewnątrz\u003C\u002Ftd\u003E\n\u003Ctd\u003Eadv:pos\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EUnii\u003C\u002Ftd\u003E\n\u003Ctd\u003Eunia\u003C\u002Ftd\u003E\n\u003Ctd\u003Esubst:sg:gen:f\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EEuropejskiej\u003C\u002Ftd\u003E\n\u003Ctd\u003Eeuropejski\u003C\u002Ftd\u003E\n\u003Ctd\u003Eadj:sg:gen:f:pos\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E.\u003C\u002Ftd\u003E\n\u003Ctd\u003E.\u003C\u002Ftd\u003E\n\u003Ctd\u003Einterp\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003EWe will not be implementing this part from scratch, instead we can use resources from IPI PAN (Polish Academy of Science), specifically a tool called \u003Ca href=\"http:\u002F\u002Fmorfeusz.sgjp.pl\u002F\"\u003EMorfeusz\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\n\u003Cp\u003EMorfeusz is a morphosyntactic analyzer which you can use to find all word lexems and the forms. The output will be slightly different than the table above: instead, for every input word Morfeusz will output all of it&#39;s possible dictionary forms.\u003C\u002Fp\u003E\n\u003Ch3 id=\"usage\"\u003EUsage\u003C\u002Fh3\u003E\n\u003Cp\u003EI assume you are running either a recent Ubuntu or Fedora. After fetching the right version from \u003Ca href=\"http:\u002F\u002Fmorfeusz.sgjp.pl\u002Fdownload\u002F\"\u003EMorfeusz download page\u003C\u002Fa\u003E do the following:\u003C\u002Fp\u003E\n\u003Cpre class='language-javascriptreact'\u003E\u003Ccode\u003Etar xzfv &lt;path to archive&gt;\nsudo cp morfeusz\u002Flib\u002Flibmorfeusz2.so \u002Fusr\u002Flib\u002F\u003Cspan class=\"hljs-built_in\"\u003Elocal\u003C\u002Fspan\u003E\nsudo chmod a+x \u002Fusr\u002Flib\u002F\u003Cspan class=\"hljs-built_in\"\u003Elocal\u003C\u002Fspan\u003E\u002Flibmorfeusz2.so\nsudo \u003Cspan class=\"hljs-built_in\"\u003Eecho\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-string\"\u003E\"\u002Fusr\u002Flocal\u002Flib\"\u003C\u002Fspan\u003E &gt; \u002Fetc\u002Fld.so.conf.d\u002Flocal.conf\nsudo ldconfig\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003ENow we can download and install the python egg from morfeusz download page. \u003C\u002Fp\u003E\n\u003Cpre class='language-javascriptreact'\u003E\u003Ccode\u003Eeasy_install &lt;path_to_downloaded_egg&gt;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EThe best thing about the python package is that you can retrieve the result from the analyzer using just a couple of python lines:\u003C\u002Fp\u003E\n\u003Cpre class='language-javascriptreact'\u003E\u003Ccode\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E morfeusz2\nm = morfeusz2.Morfeusz()\nresult = m.analyze(\u003Cspan class=\"hljs-string\"\u003E\"Ala ma kota, kot ma Alę.\"\u003C\u002Fspan\u003E)\n\u003Cspan class=\"hljs-comment\"\u003E# the result is:\u003C\u002Fspan\u003E\n[(\u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E, (\u003Cspan class=\"hljs-string\"\u003E'Dyrektywa'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'dyrektywa'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'subst:sg:nom:f'\u003C\u002Fspan\u003E, [\u003Cspan class=\"hljs-string\"\u003E'nazwa_pospolita'\u003C\u002Fspan\u003E], [])),   \n (\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E, (\u003Cspan class=\"hljs-string\"\u003E'PSD2'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'PSD2'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'ign'\u003C\u002Fspan\u003E, [], [])),                                         \n (\u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E, (\u003Cspan class=\"hljs-string\"\u003E'zawiera'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'zawierać'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'fin:sg:ter:imperf'\u003C\u002Fspan\u003E, [], [])),                    \n (\u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E,                                                                              \n  \u003Cspan class=\"hljs-number\"\u003E4\u003C\u002Fspan\u003E,                                                                              \n  (\u003Cspan class=\"hljs-string\"\u003E'przepisy'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'przepis'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'subst:pl:nom.acc.voc:m3'\u003C\u002Fspan\u003E, [\u003Cspan class=\"hljs-string\"\u003E'nazwa_pospolita'\u003C\u002Fspan\u003E], [])),   \n (\u003Cspan class=\"hljs-number\"\u003E4\u003C\u002Fspan\u003E,                                     \n  \u003Cspan class=\"hljs-number\"\u003E5\u003C\u002Fspan\u003E,                                     \n  (\u003Cspan class=\"hljs-string\"\u003E'odnoszące'\u003C\u002Fspan\u003E,                                                                   \n   \u003Cspan class=\"hljs-string\"\u003E'odnosić'\u003C\u002Fspan\u003E,                                                                     \n   \u003Cspan class=\"hljs-string\"\u003E'pact:pl:nom.acc.voc:m2.m3.f.n:imperf:aff'\u003C\u002Fspan\u003E,                                    \n   [],                                                                            \n   [])),                                 \n (\u003Cspan class=\"hljs-number\"\u003E4\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E5\u003C\u002Fspan\u003E, (\u003Cspan class=\"hljs-string\"\u003E'odnoszące'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'odnosić'\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E'pact:sg:nom.acc.voc:n:imperf:aff'\u003C\u002Fspan\u003E, [], [])),    \u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EPutting it all in a pandas DataFrame is a trivial task as well:\u003C\u002Fp\u003E\n\u003Cpre class='language-javascriptreact'\u003E\u003Ccode\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E pandas \u003Cspan class=\"hljs-keyword\"\u003Eas\u003C\u002Fspan\u003E pd\ncolnames = [\u003Cspan class=\"hljs-string\"\u003E\"token_start\"\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E\"token_end\"\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E\"segment\"\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E\"lemma\"\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E\"interp\"\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E\"common\"\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-string\"\u003E\"qualifiers\"\u003C\u002Fspan\u003E]\ndf = pd.DataFrame.from_records([(elem[\u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E], elem[\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E], *elem[\u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E]) \u003Cspan class=\"hljs-keyword\"\u003Efor\u003C\u002Fspan\u003E elem \u003Cspan class=\"hljs-keyword\"\u003Ein\u003C\u002Fspan\u003E result], columns=colnames)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cimg src=\"\u002Fimages\u002F1-assets\u002Fpandas_df.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EHelpful links on tackling Morfeusz:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fdownload.sgjp.pl\u002Fmorfeusz\u002FMorfeusz2.pdf\"\u003EFull docs on Morfeusz\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fwww.ipipan.waw.pl\u002F~wolinski\u002Fpubl\u002Fznakowanie.pdf\"\u003EMeaning of the morphosyntactic tags\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"lemmatization\"\u003ELemmatization\u003C\u002Fh3\u003E\n\u003Cp\u003EYou have already seen the output of a lemmatizer at the beginning of previous section. In contrast to Morfeusz&#39;s output, this time we want to obtain a mapping for each word to it&#39;s most probable dictionary form.\u003C\u002Fp\u003E\n\u003Cp\u003EIn order to obtain direct tags instead of list of tags for each token, you will need to go deep into morphological taggers land. Your options are:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fnlp.pwr.wroc.pl\u002Fredmine\u002Fprojects\u002Fwcrft\u002Fwiki\u002F\"\u003EWCRFT\u003C\u002Fa\u003E - actually having a working demo \u003Ca href=\"http:\u002F\u002Fws.clarin-pl.eu\u002Ftager.shtml\"\u003Ehere\u003C\u002Fa\u003E. Requires some skill to get it to work, dependencies are non-trivial to put together,\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fzil.ipipan.waw.pl\u002FPANTERA\"\u003EPANTERA\u003C\u002Fa\u003E - doesn&#39;t appear to be actively maintained\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fkawu\u002Fconcraft-pl\"\u003EConcraft\u003C\u002Fa\u003E - implemented in Haskell, looks to be the easiest one to get running\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EIf you are just doing a casual analysis of text, you may want to consider outsourcing the whole tagging process to a RESTful API, like \u003Ca href=\"http:\u002F\u002Fnlp.pwr.wroc.pl\u002Fredmine\u002Fprojects\u002Fnlprest2\u002Fwiki\u002FAsynapi\"\u003Ehttp:\u002F\u002Fnlp.pwr.wroc.pl\u002Fredmine\u002Fprojects\u002Fnlprest2\u002Fwiki\u002FAsynapi\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\n\u003Cp\u003EYou can easily reverse-engineer the exact calls to the API using Firefox\u002FChrome dev tools. This way you can find the correct values for the parameters.\n\u003Cimg src=\"\u002Fimages\u002F1-assets\u002Fdevtools.png\" alt=\"devtools\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EAs with any API provided to the public, you should notify the provider about your intentions and remember about some sane throttling of requests. Moreover, it&#39;s considered good manners to set the user parameter to something meaningful (like email address).\u003C\u002Fp\u003E\n\u003Ch2 id=\"conclusions\"\u003EConclusions\u003C\u002Fh2\u003E\n\u003Cp\u003EWe went through the basic steps of analyzing a corpus and touched on more advanced topics, like morphological analysis and lemmatization. If you&#39;re doing a quick and dirty analysis, outsourcing those basic tasks to ready tools and APIs sounds like the best option. In case you need to run a large scale experiments, it makes more sense to run the tools on your own hardware.\nThe lemmatized text might become a powerful input to a more sophisticated models, like recurrent neural networks. \u003C\u002Fp\u003E\n\u003Ch2 id=\"citations\"\u003ECitations\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E[1] \u003Cspan id=\"cite-1\"\u003E\u003C\u002Fspan\u003E\u003Ca href=\"http:\u002F\u002Fgrzegorz.jagodzinski.prv.pl\u002Fgram\u002Fpl\u002Fgram01.html\"\u003Ehttp:\u002F\u002Fgrzegorz.jagodzinski.prv.pl: Polish grammar rules\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E[2] \u003Cspan id=\"cite-2\"\u003E\u003C\u002Fspan\u003E\u003Ca href=\"https:\u002F\u002Fpl.wikipedia.org\u002Fwiki\u002FGramatyka_j%C4%99zyka_polskiego\"\u003EWikpedia: Polish grammar\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E[3] \u003Cspan id=\"cite-3\"\u003E\u003C\u002Fspan\u003E\u003Ca href=\"https:\u002F\u002Fwww.quora.com\u002FHow-many-words-are-there-in-the-Polish-language\"\u003EQuora: how many words are the in the Polish language\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n",title:"Getting started with Natural Language Processing in Polish",slug:"1-getting-started-nlp",date:"2019-09-14T16:49:20.000Z",image:"1-assets\u002Fheader.jpg"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.e20925cb.js"}catch(e){main="/client/legacy/client.85941a71.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 